{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7000210,"sourceType":"datasetVersion","datasetId":4024114}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T09:06:41.269132Z","iopub.execute_input":"2025-03-23T09:06:41.269333Z","iopub.status.idle":"2025-03-23T09:06:46.973411Z","shell.execute_reply.started":"2025-03-23T09:06:41.269313Z","shell.execute_reply":"2025-03-23T09:06:46.972202Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu126\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install spikingjelly","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T09:08:47.591262Z","iopub.execute_input":"2025-03-23T09:08:47.591604Z","iopub.status.idle":"2025-03-23T09:08:51.583610Z","shell.execute_reply.started":"2025-03-23T09:08:47.591569Z","shell.execute_reply":"2025-03-23T09:08:51.582751Z"}},"outputs":[{"name":"stdout","text":"Collecting spikingjelly\n  Downloading spikingjelly-0.0.0.0.14-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (2.5.1+cu121)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (3.7.5)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (1.26.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (4.67.1)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (0.20.1+cu121)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spikingjelly) (1.13.1)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->spikingjelly) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->spikingjelly) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->spikingjelly) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->spikingjelly) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->spikingjelly) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->spikingjelly) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->spikingjelly) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->spikingjelly) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->spikingjelly) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->spikingjelly) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->spikingjelly) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->spikingjelly) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->spikingjelly) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->spikingjelly) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->spikingjelly) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->spikingjelly) (2024.2.0)\nDownloading spikingjelly-0.0.0.0.14-py3-none-any.whl (437 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: spikingjelly\nSuccessfully installed spikingjelly-0.0.0.0.14\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch, os\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import datasets, models\nfrom spikingjelly.clock_driven import neuron, functional, surrogate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T10:34:01.345083Z","iopub.execute_input":"2025-03-23T10:34:01.345434Z","iopub.status.idle":"2025-03-23T10:34:01.350689Z","shell.execute_reply.started":"2025-03-23T10:34:01.345367Z","shell.execute_reply":"2025-03-23T10:34:01.349977Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"data_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.RandomRotation(20),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5])\n    ]),\n    'test': transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5], [0.5])\n    ]),\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T09:09:56.301713Z","iopub.execute_input":"2025-03-23T09:09:56.302016Z","iopub.status.idle":"2025-03-23T09:09:56.307379Z","shell.execute_reply.started":"2025-03-23T09:09:56.301994Z","shell.execute_reply":"2025-03-23T09:09:56.306716Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"data_dir = '/kaggle/input/fundus-pytorch'\nimage_datasets = {x: datasets.ImageFolder(\n    root=f\"{data_dir}/{x}\",\n    transform=data_transforms[x]\n) for x in ['train', 'test', 'val']}\n\ndataloaders = {x: DataLoader(image_datasets[x], batch_size=32, shuffle=True) for x in ['train', 'test', 'val']}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T09:06:57.281852Z","iopub.execute_input":"2025-03-23T09:06:57.282135Z","iopub.status.idle":"2025-03-23T09:07:36.551216Z","shell.execute_reply.started":"2025-03-23T09:06:57.282107Z","shell.execute_reply":"2025-03-23T09:07:36.550374Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class GlaucomaSNN(nn.Module):\n    def __init__(self, T=4):\n        super(GlaucomaSNN, self).__init__()\n        self.T = T  # Number of time steps\n\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            neuron.LIFNode(surrogate_function=surrogate.ATan()),  # Spiking activation\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            neuron.LIFNode(surrogate_function=surrogate.ATan()),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            neuron.LIFNode(surrogate_function=surrogate.ATan()),\n            nn.AdaptiveAvgPool2d((8, 8)),  # Adaptive pooling\n        )\n\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 8 * 8, 512),\n            neuron.LIFNode(surrogate_function=surrogate.ATan()),\n            nn.Dropout(0.3),\n            nn.Linear(512, 2)\n        )\n\n    def forward(self, x):\n        mem = 0\n        for t in range(self.T):  # Iterate over time steps\n            out = self.conv_layers(x)\n            out = self.fc_layers(out)\n            mem += out  # Accumulate membrane potential\n        \n        return mem / self.T  # Average across time steps","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T10:34:04.178054Z","iopub.execute_input":"2025-03-23T10:34:04.178351Z","iopub.status.idle":"2025-03-23T10:34:04.186114Z","shell.execute_reply.started":"2025-03-23T10:34:04.178327Z","shell.execute_reply":"2025-03-23T10:34:04.185320Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = GlaucomaSNN()\n\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(device)\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T10:34:04.372018Z","iopub.execute_input":"2025-03-23T10:34:04.372314Z","iopub.status.idle":"2025-03-23T10:34:04.414529Z","shell.execute_reply.started":"2025-03-23T10:34:04.372291Z","shell.execute_reply":"2025-03-23T10:34:04.413750Z"}},"outputs":[{"name":"stdout","text":"Using 2 GPUs!\ncuda\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T10:34:06.083893Z","iopub.execute_input":"2025-03-23T10:34:06.084178Z","iopub.status.idle":"2025-03-23T10:34:06.088075Z","shell.execute_reply.started":"2025-03-23T10:34:06.084156Z","shell.execute_reply":"2025-03-23T10:34:06.087320Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Training loop\ndef train_model(model, dataloaders, criterion, optimizer, num_epochs=10):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(\"-\" * 30)\n        \n        for batch_idx, (inputs, labels) in enumerate(dataloaders['train']):\n            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            if (batch_idx + 1) % 10 == 0:\n                print(f\"Batch {batch_idx+1}/{len(dataloaders['train'])} | Loss: {loss.item():.4f}\")\n        \n        epoch_loss = running_loss / len(dataloaders['train'])\n        epoch_acc = 100. * correct / total\n        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\\n\")\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for inputs, labels in dataloaders['val']:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                val_total += labels.size(0)\n                val_correct += predicted.eq(labels).sum().item()\n        \n        val_loss /= len(dataloaders['val'])\n        val_acc = 100. * val_correct / val_total\n        print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc:.2f}%\\n\")\n\ntrain_model(model, dataloaders, criterion, optimizer, num_epochs=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T10:34:07.071523Z","iopub.execute_input":"2025-03-23T10:34:07.071871Z","iopub.status.idle":"2025-03-23T14:56:40.993664Z","shell.execute_reply.started":"2025-03-23T10:34:07.071843Z","shell.execute_reply":"2025-03-23T14:56:40.992875Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n------------------------------\nBatch 10/270 | Loss: 0.7704\nBatch 20/270 | Loss: 0.6718\nBatch 30/270 | Loss: 0.5533\nBatch 40/270 | Loss: 0.6580\nBatch 50/270 | Loss: 0.6096\nBatch 60/270 | Loss: 0.7000\nBatch 70/270 | Loss: 0.6421\nBatch 80/270 | Loss: 0.6982\nBatch 90/270 | Loss: 0.6372\nBatch 100/270 | Loss: 0.6140\nBatch 110/270 | Loss: 0.6289\nBatch 120/270 | Loss: 0.5838\nBatch 130/270 | Loss: 0.5187\nBatch 140/270 | Loss: 0.6735\nBatch 150/270 | Loss: 0.7201\nBatch 160/270 | Loss: 0.5441\nBatch 170/270 | Loss: 0.5451\nBatch 180/270 | Loss: 0.6894\nBatch 190/270 | Loss: 0.6296\nBatch 200/270 | Loss: 0.7258\nBatch 210/270 | Loss: 0.6961\nBatch 220/270 | Loss: 0.5332\nBatch 230/270 | Loss: 0.5997\nBatch 240/270 | Loss: 0.4393\nBatch 250/270 | Loss: 0.5173\nBatch 260/270 | Loss: 0.5135\nBatch 270/270 | Loss: 0.4991\nEpoch 1 Loss: 0.5934 | Accuracy: 69.59%\n\nValidation Loss: 0.5457 | Validation Accuracy: 73.99%\n\nEpoch 2/50\n------------------------------\nBatch 10/270 | Loss: 0.5141\nBatch 20/270 | Loss: 0.4625\nBatch 30/270 | Loss: 0.5016\nBatch 40/270 | Loss: 0.6580\nBatch 50/270 | Loss: 0.4853\nBatch 60/270 | Loss: 0.7320\nBatch 70/270 | Loss: 0.4962\nBatch 80/270 | Loss: 0.5795\nBatch 90/270 | Loss: 0.3806\nBatch 100/270 | Loss: 0.4143\nBatch 110/270 | Loss: 0.4695\nBatch 120/270 | Loss: 0.3933\nBatch 130/270 | Loss: 0.4094\nBatch 140/270 | Loss: 0.7000\nBatch 150/270 | Loss: 0.5559\nBatch 160/270 | Loss: 0.5808\nBatch 170/270 | Loss: 0.6070\nBatch 180/270 | Loss: 0.4613\nBatch 190/270 | Loss: 0.5522\nBatch 200/270 | Loss: 0.5294\nBatch 210/270 | Loss: 0.6631\nBatch 220/270 | Loss: 0.4985\nBatch 230/270 | Loss: 0.4016\nBatch 240/270 | Loss: 0.5754\nBatch 250/270 | Loss: 0.6244\nBatch 260/270 | Loss: 0.6009\nBatch 270/270 | Loss: 0.4102\nEpoch 2 Loss: 0.5314 | Accuracy: 74.79%\n\nValidation Loss: 0.5063 | Validation Accuracy: 76.49%\n\nEpoch 3/50\n------------------------------\nBatch 10/270 | Loss: 0.4698\nBatch 20/270 | Loss: 0.3968\nBatch 30/270 | Loss: 0.5986\nBatch 40/270 | Loss: 0.6145\nBatch 50/270 | Loss: 0.5595\nBatch 60/270 | Loss: 0.4860\nBatch 70/270 | Loss: 0.3398\nBatch 80/270 | Loss: 0.4970\nBatch 90/270 | Loss: 0.4688\nBatch 100/270 | Loss: 0.5544\nBatch 110/270 | Loss: 0.3805\nBatch 120/270 | Loss: 0.5227\nBatch 130/270 | Loss: 0.6371\nBatch 140/270 | Loss: 0.6532\nBatch 150/270 | Loss: 0.4602\nBatch 160/270 | Loss: 0.5038\nBatch 170/270 | Loss: 0.4078\nBatch 180/270 | Loss: 0.6551\nBatch 190/270 | Loss: 0.4757\nBatch 200/270 | Loss: 0.6326\nBatch 210/270 | Loss: 0.4311\nBatch 220/270 | Loss: 0.6737\nBatch 230/270 | Loss: 0.4643\nBatch 240/270 | Loss: 0.3911\nBatch 250/270 | Loss: 0.5275\nBatch 260/270 | Loss: 0.4613\nBatch 270/270 | Loss: 0.5859\nEpoch 3 Loss: 0.5105 | Accuracy: 76.16%\n\nValidation Loss: 0.5101 | Validation Accuracy: 76.14%\n\nEpoch 4/50\n------------------------------\nBatch 10/270 | Loss: 0.5777\nBatch 20/270 | Loss: 0.6247\nBatch 30/270 | Loss: 0.3553\nBatch 40/270 | Loss: 0.3747\nBatch 50/270 | Loss: 0.6835\nBatch 60/270 | Loss: 0.4813\nBatch 70/270 | Loss: 0.5701\nBatch 80/270 | Loss: 0.4971\nBatch 90/270 | Loss: 0.6495\nBatch 100/270 | Loss: 0.5942\nBatch 110/270 | Loss: 0.3433\nBatch 120/270 | Loss: 0.5068\nBatch 130/270 | Loss: 0.3731\nBatch 140/270 | Loss: 0.5954\nBatch 150/270 | Loss: 0.3503\nBatch 160/270 | Loss: 0.3613\nBatch 170/270 | Loss: 0.6020\nBatch 180/270 | Loss: 0.4046\nBatch 190/270 | Loss: 0.5321\nBatch 200/270 | Loss: 0.4190\nBatch 210/270 | Loss: 0.4836\nBatch 220/270 | Loss: 0.4813\nBatch 230/270 | Loss: 0.5762\nBatch 240/270 | Loss: 0.5351\nBatch 250/270 | Loss: 0.4414\nBatch 260/270 | Loss: 0.4907\nBatch 270/270 | Loss: 0.4448\nEpoch 4 Loss: 0.4923 | Accuracy: 77.02%\n\nValidation Loss: 0.5708 | Validation Accuracy: 71.34%\n\nEpoch 5/50\n------------------------------\nBatch 10/270 | Loss: 0.3688\nBatch 20/270 | Loss: 0.5189\nBatch 30/270 | Loss: 0.4847\nBatch 40/270 | Loss: 0.5578\nBatch 50/270 | Loss: 0.3959\nBatch 60/270 | Loss: 0.4853\nBatch 70/270 | Loss: 0.5572\nBatch 80/270 | Loss: 0.5610\nBatch 90/270 | Loss: 0.4272\nBatch 100/270 | Loss: 0.5147\nBatch 110/270 | Loss: 0.4915\nBatch 120/270 | Loss: 0.5296\nBatch 130/270 | Loss: 0.3507\nBatch 140/270 | Loss: 0.4222\nBatch 150/270 | Loss: 0.4198\nBatch 160/270 | Loss: 0.3809\nBatch 170/270 | Loss: 0.4360\nBatch 180/270 | Loss: 0.4564\nBatch 190/270 | Loss: 0.4867\nBatch 200/270 | Loss: 0.3959\nBatch 210/270 | Loss: 0.3780\nBatch 220/270 | Loss: 0.4088\nBatch 230/270 | Loss: 0.5588\nBatch 240/270 | Loss: 0.7045\nBatch 250/270 | Loss: 0.5101\nBatch 260/270 | Loss: 0.4731\nBatch 270/270 | Loss: 0.5045\nEpoch 5 Loss: 0.4863 | Accuracy: 77.22%\n\nValidation Loss: 0.5130 | Validation Accuracy: 74.18%\n\nEpoch 6/50\n------------------------------\nBatch 10/270 | Loss: 0.6456\nBatch 20/270 | Loss: 0.4933\nBatch 30/270 | Loss: 0.4441\nBatch 40/270 | Loss: 0.3241\nBatch 50/270 | Loss: 0.4504\nBatch 60/270 | Loss: 0.3438\nBatch 70/270 | Loss: 0.5596\nBatch 80/270 | Loss: 0.4553\nBatch 90/270 | Loss: 0.4084\nBatch 100/270 | Loss: 0.4700\nBatch 110/270 | Loss: 0.3323\nBatch 120/270 | Loss: 0.5292\nBatch 130/270 | Loss: 0.5528\nBatch 140/270 | Loss: 0.4919\nBatch 150/270 | Loss: 0.5400\nBatch 160/270 | Loss: 0.3795\nBatch 170/270 | Loss: 0.4256\nBatch 180/270 | Loss: 0.4132\nBatch 190/270 | Loss: 0.4987\nBatch 200/270 | Loss: 0.5208\nBatch 210/270 | Loss: 0.3614\nBatch 220/270 | Loss: 0.2731\nBatch 230/270 | Loss: 0.4029\nBatch 240/270 | Loss: 0.3832\nBatch 250/270 | Loss: 0.4883\nBatch 260/270 | Loss: 0.4658\nBatch 270/270 | Loss: 0.3835\nEpoch 6 Loss: 0.4708 | Accuracy: 77.86%\n\nValidation Loss: 0.4938 | Validation Accuracy: 76.46%\n\nEpoch 7/50\n------------------------------\nBatch 10/270 | Loss: 0.5764\nBatch 20/270 | Loss: 0.4283\nBatch 30/270 | Loss: 0.3312\nBatch 40/270 | Loss: 0.6192\nBatch 50/270 | Loss: 0.5416\nBatch 60/270 | Loss: 0.4806\nBatch 70/270 | Loss: 0.4479\nBatch 80/270 | Loss: 0.4146\nBatch 90/270 | Loss: 0.6526\nBatch 100/270 | Loss: 0.5091\nBatch 110/270 | Loss: 0.4860\nBatch 120/270 | Loss: 0.2835\nBatch 130/270 | Loss: 0.3652\nBatch 140/270 | Loss: 0.4773\nBatch 150/270 | Loss: 0.4136\nBatch 160/270 | Loss: 0.5138\nBatch 170/270 | Loss: 0.4992\nBatch 180/270 | Loss: 0.4661\nBatch 190/270 | Loss: 0.4364\nBatch 200/270 | Loss: 0.6211\nBatch 210/270 | Loss: 0.4342\nBatch 220/270 | Loss: 0.3394\nBatch 230/270 | Loss: 0.7176\nBatch 240/270 | Loss: 0.7332\nBatch 250/270 | Loss: 0.4437\nBatch 260/270 | Loss: 0.3360\nBatch 270/270 | Loss: 0.3701\nEpoch 7 Loss: 0.4561 | Accuracy: 78.95%\n\nValidation Loss: 0.4628 | Validation Accuracy: 78.48%\n\nEpoch 8/50\n------------------------------\nBatch 10/270 | Loss: 0.3716\nBatch 20/270 | Loss: 0.3709\nBatch 30/270 | Loss: 0.4314\nBatch 40/270 | Loss: 0.5583\nBatch 50/270 | Loss: 0.5714\nBatch 60/270 | Loss: 0.4925\nBatch 70/270 | Loss: 0.5752\nBatch 80/270 | Loss: 0.4492\nBatch 90/270 | Loss: 0.4212\nBatch 100/270 | Loss: 0.5403\nBatch 110/270 | Loss: 0.5484\nBatch 120/270 | Loss: 0.4928\nBatch 130/270 | Loss: 0.6313\nBatch 140/270 | Loss: 0.4639\nBatch 150/270 | Loss: 0.5725\nBatch 160/270 | Loss: 0.3069\nBatch 170/270 | Loss: 0.3579\nBatch 180/270 | Loss: 0.3489\nBatch 190/270 | Loss: 0.6391\nBatch 200/270 | Loss: 0.5472\nBatch 210/270 | Loss: 0.4362\nBatch 220/270 | Loss: 0.3679\nBatch 230/270 | Loss: 0.4829\nBatch 240/270 | Loss: 0.4175\nBatch 250/270 | Loss: 0.4311\nBatch 260/270 | Loss: 0.4658\nBatch 270/270 | Loss: 0.5759\nEpoch 8 Loss: 0.4558 | Accuracy: 78.98%\n\nValidation Loss: 0.4550 | Validation Accuracy: 79.21%\n\nEpoch 9/50\n------------------------------\nBatch 10/270 | Loss: 0.3908\nBatch 20/270 | Loss: 0.4836\nBatch 30/270 | Loss: 0.4378\nBatch 40/270 | Loss: 0.3242\nBatch 50/270 | Loss: 0.4926\nBatch 60/270 | Loss: 0.3721\nBatch 70/270 | Loss: 0.4181\nBatch 80/270 | Loss: 0.4877\nBatch 90/270 | Loss: 0.3326\nBatch 100/270 | Loss: 0.3910\nBatch 110/270 | Loss: 0.4561\nBatch 120/270 | Loss: 0.4572\nBatch 130/270 | Loss: 0.4677\nBatch 140/270 | Loss: 0.6473\nBatch 150/270 | Loss: 0.5156\nBatch 160/270 | Loss: 0.3355\nBatch 170/270 | Loss: 0.3977\nBatch 180/270 | Loss: 0.4733\nBatch 190/270 | Loss: 0.6002\nBatch 200/270 | Loss: 0.4130\nBatch 210/270 | Loss: 0.4386\nBatch 220/270 | Loss: 0.3976\nBatch 230/270 | Loss: 0.4786\nBatch 240/270 | Loss: 0.3597\nBatch 250/270 | Loss: 0.4176\nBatch 260/270 | Loss: 0.3643\nBatch 270/270 | Loss: 0.4018\nEpoch 9 Loss: 0.4430 | Accuracy: 79.43%\n\nValidation Loss: 0.4454 | Validation Accuracy: 79.02%\n\nEpoch 10/50\n------------------------------\nBatch 10/270 | Loss: 0.3532\nBatch 20/270 | Loss: 0.3613\nBatch 30/270 | Loss: 0.5128\nBatch 40/270 | Loss: 0.4052\nBatch 50/270 | Loss: 0.3905\nBatch 60/270 | Loss: 0.4310\nBatch 70/270 | Loss: 0.5032\nBatch 80/270 | Loss: 0.4427\nBatch 90/270 | Loss: 0.3458\nBatch 100/270 | Loss: 0.4587\nBatch 110/270 | Loss: 0.3866\nBatch 120/270 | Loss: 0.3524\nBatch 130/270 | Loss: 0.4835\nBatch 140/270 | Loss: 0.3856\nBatch 150/270 | Loss: 0.5465\nBatch 160/270 | Loss: 0.4431\nBatch 170/270 | Loss: 0.4776\nBatch 180/270 | Loss: 0.2776\nBatch 190/270 | Loss: 0.5175\nBatch 200/270 | Loss: 0.3818\nBatch 210/270 | Loss: 0.4900\nBatch 220/270 | Loss: 0.4325\nBatch 230/270 | Loss: 0.4678\nBatch 240/270 | Loss: 0.6777\nBatch 250/270 | Loss: 0.3339\nBatch 260/270 | Loss: 0.4388\nBatch 270/270 | Loss: 0.2220\nEpoch 10 Loss: 0.4411 | Accuracy: 79.26%\n\nValidation Loss: 0.4499 | Validation Accuracy: 79.29%\n\nEpoch 11/50\n------------------------------\nBatch 10/270 | Loss: 0.3942\nBatch 20/270 | Loss: 0.4622\nBatch 30/270 | Loss: 0.4507\nBatch 40/270 | Loss: 0.4678\nBatch 50/270 | Loss: 0.4606\nBatch 60/270 | Loss: 0.4039\nBatch 70/270 | Loss: 0.4178\nBatch 80/270 | Loss: 0.5654\nBatch 90/270 | Loss: 0.4604\nBatch 100/270 | Loss: 0.4111\nBatch 110/270 | Loss: 0.3665\nBatch 120/270 | Loss: 0.3084\nBatch 130/270 | Loss: 0.4817\nBatch 140/270 | Loss: 0.4074\nBatch 150/270 | Loss: 0.5653\nBatch 160/270 | Loss: 0.1966\nBatch 170/270 | Loss: 0.3480\nBatch 180/270 | Loss: 0.4995\nBatch 190/270 | Loss: 0.5082\nBatch 200/270 | Loss: 0.4340\nBatch 210/270 | Loss: 0.4709\nBatch 220/270 | Loss: 0.3227\nBatch 230/270 | Loss: 0.4736\nBatch 240/270 | Loss: 0.4823\nBatch 250/270 | Loss: 0.3063\nBatch 260/270 | Loss: 0.5659\nBatch 270/270 | Loss: 0.4683\nEpoch 11 Loss: 0.4438 | Accuracy: 79.48%\n\nValidation Loss: 0.4339 | Validation Accuracy: 80.88%\n\nEpoch 12/50\n------------------------------\nBatch 10/270 | Loss: 0.6562\nBatch 20/270 | Loss: 0.4242\nBatch 30/270 | Loss: 0.3599\nBatch 40/270 | Loss: 0.4261\nBatch 50/270 | Loss: 0.4630\nBatch 60/270 | Loss: 0.3862\nBatch 70/270 | Loss: 0.3923\nBatch 80/270 | Loss: 0.7271\nBatch 90/270 | Loss: 0.3495\nBatch 100/270 | Loss: 0.4518\nBatch 110/270 | Loss: 0.5659\nBatch 120/270 | Loss: 0.2676\nBatch 130/270 | Loss: 0.3885\nBatch 140/270 | Loss: 0.4318\nBatch 150/270 | Loss: 0.3603\nBatch 160/270 | Loss: 0.3908\nBatch 170/270 | Loss: 0.4565\nBatch 180/270 | Loss: 0.4401\nBatch 190/270 | Loss: 0.3463\nBatch 200/270 | Loss: 0.3599\nBatch 210/270 | Loss: 0.5133\nBatch 220/270 | Loss: 0.3495\nBatch 230/270 | Loss: 0.2732\nBatch 240/270 | Loss: 0.3462\nBatch 250/270 | Loss: 0.4499\nBatch 260/270 | Loss: 0.5527\nBatch 270/270 | Loss: 0.6812\nEpoch 12 Loss: 0.4337 | Accuracy: 80.21%\n\nValidation Loss: 0.4166 | Validation Accuracy: 81.49%\n\nEpoch 13/50\n------------------------------\nBatch 10/270 | Loss: 0.4068\nBatch 20/270 | Loss: 0.2802\nBatch 30/270 | Loss: 0.4249\nBatch 40/270 | Loss: 0.6168\nBatch 50/270 | Loss: 0.3949\nBatch 60/270 | Loss: 0.4147\nBatch 70/270 | Loss: 0.4434\nBatch 80/270 | Loss: 0.3307\nBatch 90/270 | Loss: 0.3437\nBatch 100/270 | Loss: 0.4267\nBatch 110/270 | Loss: 0.3713\nBatch 120/270 | Loss: 0.5073\nBatch 130/270 | Loss: 0.4248\nBatch 140/270 | Loss: 0.4489\nBatch 150/270 | Loss: 0.4127\nBatch 160/270 | Loss: 0.3882\nBatch 170/270 | Loss: 0.2967\nBatch 180/270 | Loss: 0.3606\nBatch 190/270 | Loss: 0.5066\nBatch 200/270 | Loss: 0.3895\nBatch 210/270 | Loss: 0.2396\nBatch 220/270 | Loss: 0.4281\nBatch 230/270 | Loss: 0.4064\nBatch 240/270 | Loss: 0.3033\nBatch 250/270 | Loss: 0.3987\nBatch 260/270 | Loss: 0.2196\nBatch 270/270 | Loss: 0.6593\nEpoch 13 Loss: 0.4223 | Accuracy: 80.97%\n\nValidation Loss: 0.4204 | Validation Accuracy: 81.54%\n\nEpoch 14/50\n------------------------------\nBatch 10/270 | Loss: 0.4390\nBatch 20/270 | Loss: 0.4652\nBatch 30/270 | Loss: 0.2957\nBatch 40/270 | Loss: 0.4627\nBatch 50/270 | Loss: 0.3382\nBatch 60/270 | Loss: 0.3588\nBatch 70/270 | Loss: 0.4937\nBatch 80/270 | Loss: 0.3555\nBatch 90/270 | Loss: 0.4758\nBatch 100/270 | Loss: 0.4830\nBatch 110/270 | Loss: 0.3549\nBatch 120/270 | Loss: 0.3883\nBatch 130/270 | Loss: 0.2943\nBatch 140/270 | Loss: 0.4087\nBatch 150/270 | Loss: 0.4619\nBatch 160/270 | Loss: 0.3567\nBatch 170/270 | Loss: 0.4383\nBatch 180/270 | Loss: 0.4292\nBatch 190/270 | Loss: 0.3729\nBatch 200/270 | Loss: 0.2947\nBatch 210/270 | Loss: 0.4375\nBatch 220/270 | Loss: 0.4268\nBatch 230/270 | Loss: 0.4419\nBatch 240/270 | Loss: 0.4663\nBatch 250/270 | Loss: 0.3916\nBatch 260/270 | Loss: 0.4369\nBatch 270/270 | Loss: 0.3946\nEpoch 14 Loss: 0.4236 | Accuracy: 80.47%\n\nValidation Loss: 0.4112 | Validation Accuracy: 81.47%\n\nEpoch 15/50\n------------------------------\nBatch 10/270 | Loss: 0.3497\nBatch 20/270 | Loss: 0.3780\nBatch 30/270 | Loss: 0.5536\nBatch 40/270 | Loss: 0.3604\nBatch 50/270 | Loss: 0.4392\nBatch 60/270 | Loss: 0.3081\nBatch 70/270 | Loss: 0.3389\nBatch 80/270 | Loss: 0.4924\nBatch 90/270 | Loss: 0.5434\nBatch 100/270 | Loss: 0.4803\nBatch 110/270 | Loss: 0.3439\nBatch 120/270 | Loss: 0.5179\nBatch 130/270 | Loss: 0.4627\nBatch 140/270 | Loss: 0.3547\nBatch 150/270 | Loss: 0.3288\nBatch 160/270 | Loss: 0.4176\nBatch 170/270 | Loss: 0.4714\nBatch 180/270 | Loss: 0.4384\nBatch 190/270 | Loss: 0.6062\nBatch 200/270 | Loss: 0.3909\nBatch 210/270 | Loss: 0.3726\nBatch 220/270 | Loss: 0.5970\nBatch 230/270 | Loss: 0.3670\nBatch 240/270 | Loss: 0.3140\nBatch 250/270 | Loss: 0.3906\nBatch 260/270 | Loss: 0.3479\nBatch 270/270 | Loss: 0.2856\nEpoch 15 Loss: 0.4125 | Accuracy: 81.20%\n\nValidation Loss: 0.3975 | Validation Accuracy: 81.85%\n\nEpoch 16/50\n------------------------------\nBatch 10/270 | Loss: 0.3181\nBatch 20/270 | Loss: 0.3342\nBatch 30/270 | Loss: 0.2895\nBatch 40/270 | Loss: 0.2264\nBatch 50/270 | Loss: 0.4387\nBatch 60/270 | Loss: 0.4460\nBatch 70/270 | Loss: 0.2366\nBatch 80/270 | Loss: 0.4288\nBatch 90/270 | Loss: 0.3516\nBatch 100/270 | Loss: 0.3025\nBatch 110/270 | Loss: 0.3060\nBatch 120/270 | Loss: 0.3960\nBatch 130/270 | Loss: 0.7300\nBatch 140/270 | Loss: 0.3934\nBatch 150/270 | Loss: 0.6080\nBatch 160/270 | Loss: 0.3593\nBatch 170/270 | Loss: 0.7623\nBatch 180/270 | Loss: 0.4400\nBatch 190/270 | Loss: 0.3514\nBatch 200/270 | Loss: 0.3126\nBatch 210/270 | Loss: 0.2986\nBatch 220/270 | Loss: 0.3971\nBatch 230/270 | Loss: 0.2484\nBatch 240/270 | Loss: 0.3431\nBatch 250/270 | Loss: 0.3468\nBatch 260/270 | Loss: 0.5021\nBatch 270/270 | Loss: 0.2393\nEpoch 16 Loss: 0.4072 | Accuracy: 81.50%\n\nValidation Loss: 0.3986 | Validation Accuracy: 81.42%\n\nEpoch 17/50\n------------------------------\nBatch 10/270 | Loss: 0.4750\nBatch 20/270 | Loss: 0.2213\nBatch 30/270 | Loss: 0.5908\nBatch 40/270 | Loss: 0.1957\nBatch 50/270 | Loss: 0.3825\nBatch 60/270 | Loss: 0.5501\nBatch 70/270 | Loss: 0.5768\nBatch 80/270 | Loss: 0.4173\nBatch 90/270 | Loss: 0.5529\nBatch 100/270 | Loss: 0.2990\nBatch 110/270 | Loss: 0.3462\nBatch 120/270 | Loss: 0.3928\nBatch 130/270 | Loss: 0.2906\nBatch 140/270 | Loss: 0.3285\nBatch 150/270 | Loss: 0.2895\nBatch 160/270 | Loss: 0.3937\nBatch 170/270 | Loss: 0.4720\nBatch 180/270 | Loss: 0.5123\nBatch 190/270 | Loss: 0.3304\nBatch 200/270 | Loss: 0.3767\nBatch 210/270 | Loss: 0.3305\nBatch 220/270 | Loss: 0.3660\nBatch 230/270 | Loss: 0.4919\nBatch 240/270 | Loss: 0.3762\nBatch 250/270 | Loss: 0.5082\nBatch 260/270 | Loss: 0.4562\nBatch 270/270 | Loss: 0.1682\nEpoch 17 Loss: 0.4046 | Accuracy: 81.63%\n\nValidation Loss: 0.3909 | Validation Accuracy: 83.35%\n\nEpoch 18/50\n------------------------------\nBatch 10/270 | Loss: 0.5500\nBatch 20/270 | Loss: 0.2299\nBatch 30/270 | Loss: 0.2956\nBatch 40/270 | Loss: 0.3752\nBatch 50/270 | Loss: 0.4600\nBatch 60/270 | Loss: 0.4272\nBatch 70/270 | Loss: 0.2896\nBatch 80/270 | Loss: 0.3569\nBatch 90/270 | Loss: 0.3152\nBatch 100/270 | Loss: 0.4107\nBatch 110/270 | Loss: 0.3739\nBatch 120/270 | Loss: 0.5721\nBatch 130/270 | Loss: 0.4241\nBatch 140/270 | Loss: 0.4000\nBatch 150/270 | Loss: 0.3867\nBatch 160/270 | Loss: 0.4315\nBatch 170/270 | Loss: 0.6010\nBatch 180/270 | Loss: 0.3546\nBatch 190/270 | Loss: 0.3643\nBatch 200/270 | Loss: 0.3925\nBatch 210/270 | Loss: 0.2912\nBatch 220/270 | Loss: 0.2197\nBatch 230/270 | Loss: 0.2966\nBatch 240/270 | Loss: 0.5724\nBatch 250/270 | Loss: 0.4808\nBatch 260/270 | Loss: 0.2357\nBatch 270/270 | Loss: 0.1283\nEpoch 18 Loss: 0.4006 | Accuracy: 81.74%\n\nValidation Loss: 0.4176 | Validation Accuracy: 81.22%\n\nEpoch 19/50\n------------------------------\nBatch 10/270 | Loss: 0.3233\nBatch 20/270 | Loss: 0.5346\nBatch 30/270 | Loss: 0.3826\nBatch 40/270 | Loss: 0.3453\nBatch 50/270 | Loss: 0.5456\nBatch 60/270 | Loss: 0.2988\nBatch 70/270 | Loss: 0.2690\nBatch 80/270 | Loss: 0.4328\nBatch 90/270 | Loss: 0.4102\nBatch 100/270 | Loss: 0.5224\nBatch 110/270 | Loss: 0.5399\nBatch 120/270 | Loss: 0.3550\nBatch 130/270 | Loss: 0.2951\nBatch 140/270 | Loss: 0.2471\nBatch 150/270 | Loss: 0.6406\nBatch 160/270 | Loss: 0.5956\nBatch 170/270 | Loss: 0.3145\nBatch 180/270 | Loss: 0.2504\nBatch 190/270 | Loss: 0.3560\nBatch 200/270 | Loss: 0.4769\nBatch 210/270 | Loss: 0.3261\nBatch 220/270 | Loss: 0.4324\nBatch 230/270 | Loss: 0.4027\nBatch 240/270 | Loss: 0.4538\nBatch 250/270 | Loss: 0.4702\nBatch 260/270 | Loss: 0.6282\nBatch 270/270 | Loss: 0.3692\nEpoch 19 Loss: 0.3942 | Accuracy: 82.08%\n\nValidation Loss: 0.3946 | Validation Accuracy: 81.90%\n\nEpoch 20/50\n------------------------------\nBatch 10/270 | Loss: 0.3525\nBatch 20/270 | Loss: 0.5925\nBatch 30/270 | Loss: 0.3207\nBatch 40/270 | Loss: 0.4039\nBatch 50/270 | Loss: 0.4332\nBatch 60/270 | Loss: 0.3101\nBatch 70/270 | Loss: 0.4071\nBatch 80/270 | Loss: 0.4345\nBatch 90/270 | Loss: 0.3665\nBatch 100/270 | Loss: 0.5070\nBatch 110/270 | Loss: 0.4837\nBatch 120/270 | Loss: 0.3589\nBatch 130/270 | Loss: 0.3758\nBatch 140/270 | Loss: 0.2326\nBatch 150/270 | Loss: 0.2500\nBatch 160/270 | Loss: 0.2601\nBatch 170/270 | Loss: 0.5037\nBatch 180/270 | Loss: 0.5131\nBatch 190/270 | Loss: 0.3329\nBatch 200/270 | Loss: 0.3017\nBatch 210/270 | Loss: 0.3549\nBatch 220/270 | Loss: 0.3542\nBatch 230/270 | Loss: 0.3226\nBatch 240/270 | Loss: 0.3921\nBatch 250/270 | Loss: 0.3737\nBatch 260/270 | Loss: 0.2752\nBatch 270/270 | Loss: 0.3254\nEpoch 20 Loss: 0.3845 | Accuracy: 82.37%\n\nValidation Loss: 0.3754 | Validation Accuracy: 83.45%\n\nEpoch 21/50\n------------------------------\nBatch 10/270 | Loss: 0.4349\nBatch 20/270 | Loss: 0.4872\nBatch 30/270 | Loss: 0.4281\nBatch 40/270 | Loss: 0.5631\nBatch 50/270 | Loss: 0.2859\nBatch 60/270 | Loss: 0.3543\nBatch 70/270 | Loss: 0.5283\nBatch 80/270 | Loss: 0.4101\nBatch 90/270 | Loss: 0.2544\nBatch 100/270 | Loss: 0.2448\nBatch 110/270 | Loss: 0.3668\nBatch 120/270 | Loss: 0.3438\nBatch 130/270 | Loss: 0.4505\nBatch 140/270 | Loss: 0.3999\nBatch 150/270 | Loss: 0.4191\nBatch 160/270 | Loss: 0.2367\nBatch 170/270 | Loss: 0.2680\nBatch 180/270 | Loss: 0.3698\nBatch 190/270 | Loss: 0.4584\nBatch 200/270 | Loss: 0.4408\nBatch 210/270 | Loss: 0.3379\nBatch 220/270 | Loss: 0.3098\nBatch 230/270 | Loss: 0.4178\nBatch 240/270 | Loss: 0.3616\nBatch 250/270 | Loss: 0.5118\nBatch 260/270 | Loss: 0.5099\nBatch 270/270 | Loss: 0.3918\nEpoch 21 Loss: 0.3923 | Accuracy: 81.86%\n\nValidation Loss: 0.3927 | Validation Accuracy: 81.78%\n\nEpoch 22/50\n------------------------------\nBatch 10/270 | Loss: 0.3266\nBatch 20/270 | Loss: 0.4544\nBatch 30/270 | Loss: 0.4558\nBatch 40/270 | Loss: 0.4164\nBatch 50/270 | Loss: 0.4318\nBatch 60/270 | Loss: 0.4309\nBatch 70/270 | Loss: 0.2863\nBatch 80/270 | Loss: 0.3242\nBatch 90/270 | Loss: 0.2763\nBatch 100/270 | Loss: 0.3426\nBatch 110/270 | Loss: 0.4006\nBatch 120/270 | Loss: 0.3224\nBatch 130/270 | Loss: 0.3137\nBatch 140/270 | Loss: 0.2086\nBatch 150/270 | Loss: 0.4717\nBatch 160/270 | Loss: 0.3182\nBatch 170/270 | Loss: 0.3440\nBatch 180/270 | Loss: 0.3439\nBatch 190/270 | Loss: 0.3406\nBatch 200/270 | Loss: 0.3369\nBatch 210/270 | Loss: 0.5204\nBatch 220/270 | Loss: 0.3330\nBatch 230/270 | Loss: 0.4569\nBatch 240/270 | Loss: 0.3636\nBatch 250/270 | Loss: 0.2855\nBatch 260/270 | Loss: 0.3604\nBatch 270/270 | Loss: 0.3347\nEpoch 22 Loss: 0.3821 | Accuracy: 82.76%\n\nValidation Loss: 0.3855 | Validation Accuracy: 83.30%\n\nEpoch 23/50\n------------------------------\nBatch 10/270 | Loss: 0.3707\nBatch 20/270 | Loss: 0.2961\nBatch 30/270 | Loss: 0.4724\nBatch 40/270 | Loss: 0.4935\nBatch 50/270 | Loss: 0.3478\nBatch 60/270 | Loss: 0.3663\nBatch 70/270 | Loss: 0.5008\nBatch 80/270 | Loss: 0.2048\nBatch 90/270 | Loss: 0.2767\nBatch 100/270 | Loss: 0.4547\nBatch 110/270 | Loss: 0.6627\nBatch 120/270 | Loss: 0.6456\nBatch 130/270 | Loss: 0.3970\nBatch 140/270 | Loss: 0.3314\nBatch 150/270 | Loss: 0.3087\nBatch 160/270 | Loss: 0.3514\nBatch 170/270 | Loss: 0.6080\nBatch 180/270 | Loss: 0.2698\nBatch 190/270 | Loss: 0.3021\nBatch 200/270 | Loss: 0.3698\nBatch 210/270 | Loss: 0.3493\nBatch 220/270 | Loss: 0.1991\nBatch 230/270 | Loss: 0.4255\nBatch 240/270 | Loss: 0.3793\nBatch 250/270 | Loss: 0.2903\nBatch 260/270 | Loss: 0.4482\nBatch 270/270 | Loss: 0.1648\nEpoch 23 Loss: 0.3789 | Accuracy: 82.96%\n\nValidation Loss: 0.3691 | Validation Accuracy: 84.18%\n\nEpoch 24/50\n------------------------------\nBatch 10/270 | Loss: 0.3199\nBatch 20/270 | Loss: 0.4900\nBatch 30/270 | Loss: 0.4141\nBatch 40/270 | Loss: 0.3691\nBatch 50/270 | Loss: 0.4288\nBatch 60/270 | Loss: 0.4423\nBatch 70/270 | Loss: 0.2630\nBatch 80/270 | Loss: 0.5751\nBatch 90/270 | Loss: 0.3189\nBatch 100/270 | Loss: 0.2889\nBatch 110/270 | Loss: 0.4642\nBatch 120/270 | Loss: 0.3228\nBatch 130/270 | Loss: 0.7038\nBatch 140/270 | Loss: 0.5413\nBatch 150/270 | Loss: 0.4566\nBatch 160/270 | Loss: 0.3311\nBatch 170/270 | Loss: 0.3840\nBatch 180/270 | Loss: 0.3303\nBatch 190/270 | Loss: 0.3917\nBatch 200/270 | Loss: 0.1927\nBatch 210/270 | Loss: 0.5840\nBatch 220/270 | Loss: 0.2501\nBatch 230/270 | Loss: 0.3311\nBatch 240/270 | Loss: 0.3092\nBatch 250/270 | Loss: 0.3731\nBatch 260/270 | Loss: 0.2832\nBatch 270/270 | Loss: 0.3878\nEpoch 24 Loss: 0.3700 | Accuracy: 83.34%\n\nValidation Loss: 0.3624 | Validation Accuracy: 84.03%\n\nEpoch 25/50\n------------------------------\nBatch 10/270 | Loss: 0.3621\nBatch 20/270 | Loss: 0.4211\nBatch 30/270 | Loss: 0.3455\nBatch 40/270 | Loss: 0.3409\nBatch 50/270 | Loss: 0.3748\nBatch 60/270 | Loss: 0.2533\nBatch 70/270 | Loss: 0.2451\nBatch 80/270 | Loss: 0.5938\nBatch 90/270 | Loss: 0.3991\nBatch 100/270 | Loss: 0.2589\nBatch 110/270 | Loss: 0.4512\nBatch 120/270 | Loss: 0.3029\nBatch 130/270 | Loss: 0.4272\nBatch 140/270 | Loss: 0.4748\nBatch 150/270 | Loss: 0.3237\nBatch 160/270 | Loss: 0.3234\nBatch 170/270 | Loss: 0.4048\nBatch 180/270 | Loss: 0.3571\nBatch 190/270 | Loss: 0.4569\nBatch 200/270 | Loss: 0.4182\nBatch 210/270 | Loss: 0.4026\nBatch 220/270 | Loss: 0.2517\nBatch 230/270 | Loss: 0.4259\nBatch 240/270 | Loss: 0.2887\nBatch 250/270 | Loss: 0.4199\nBatch 260/270 | Loss: 0.3186\nBatch 270/270 | Loss: 0.3194\nEpoch 25 Loss: 0.3729 | Accuracy: 83.11%\n\nValidation Loss: 0.3773 | Validation Accuracy: 83.28%\n\nEpoch 26/50\n------------------------------\nBatch 10/270 | Loss: 0.1964\nBatch 20/270 | Loss: 0.4184\nBatch 30/270 | Loss: 0.3856\nBatch 40/270 | Loss: 0.3177\nBatch 50/270 | Loss: 0.3567\nBatch 60/270 | Loss: 0.4816\nBatch 70/270 | Loss: 0.6544\nBatch 80/270 | Loss: 0.3759\nBatch 90/270 | Loss: 0.4307\nBatch 100/270 | Loss: 0.3307\nBatch 110/270 | Loss: 0.4199\nBatch 120/270 | Loss: 0.4952\nBatch 130/270 | Loss: 0.4174\nBatch 140/270 | Loss: 0.2314\nBatch 150/270 | Loss: 0.5723\nBatch 160/270 | Loss: 0.3718\nBatch 170/270 | Loss: 0.1903\nBatch 180/270 | Loss: 0.3683\nBatch 190/270 | Loss: 0.2839\nBatch 200/270 | Loss: 0.2980\nBatch 210/270 | Loss: 0.4592\nBatch 220/270 | Loss: 0.2915\nBatch 230/270 | Loss: 0.1752\nBatch 240/270 | Loss: 0.6230\nBatch 250/270 | Loss: 0.5387\nBatch 260/270 | Loss: 0.3596\nBatch 270/270 | Loss: 0.3315\nEpoch 26 Loss: 0.3706 | Accuracy: 83.44%\n\nValidation Loss: 0.4367 | Validation Accuracy: 78.82%\n\nEpoch 27/50\n------------------------------\nBatch 10/270 | Loss: 0.4185\nBatch 20/270 | Loss: 0.3380\nBatch 30/270 | Loss: 0.6483\nBatch 40/270 | Loss: 0.3740\nBatch 50/270 | Loss: 0.2912\nBatch 60/270 | Loss: 0.3181\nBatch 70/270 | Loss: 0.3119\nBatch 80/270 | Loss: 0.3562\nBatch 90/270 | Loss: 0.1712\nBatch 100/270 | Loss: 0.4453\nBatch 110/270 | Loss: 0.4127\nBatch 120/270 | Loss: 0.5529\nBatch 130/270 | Loss: 0.4170\nBatch 140/270 | Loss: 0.3202\nBatch 150/270 | Loss: 0.6845\nBatch 160/270 | Loss: 0.3510\nBatch 170/270 | Loss: 0.5575\nBatch 180/270 | Loss: 0.2589\nBatch 190/270 | Loss: 0.3279\nBatch 200/270 | Loss: 0.3495\nBatch 210/270 | Loss: 0.3995\nBatch 220/270 | Loss: 0.3663\nBatch 230/270 | Loss: 0.4893\nBatch 240/270 | Loss: 0.3815\nBatch 250/270 | Loss: 0.3159\nBatch 260/270 | Loss: 0.4374\nBatch 270/270 | Loss: 0.4745\nEpoch 27 Loss: 0.3672 | Accuracy: 83.51%\n\nValidation Loss: 0.3834 | Validation Accuracy: 82.69%\n\nEpoch 28/50\n------------------------------\nBatch 10/270 | Loss: 0.3400\nBatch 20/270 | Loss: 0.2816\nBatch 30/270 | Loss: 0.3912\nBatch 40/270 | Loss: 0.3421\nBatch 50/270 | Loss: 0.3148\nBatch 60/270 | Loss: 0.2806\nBatch 70/270 | Loss: 0.2181\nBatch 80/270 | Loss: 0.5518\nBatch 90/270 | Loss: 0.3059\nBatch 100/270 | Loss: 0.2382\nBatch 110/270 | Loss: 0.4259\nBatch 120/270 | Loss: 0.3763\nBatch 130/270 | Loss: 0.2719\nBatch 140/270 | Loss: 0.3333\nBatch 150/270 | Loss: 0.3164\nBatch 160/270 | Loss: 0.3116\nBatch 170/270 | Loss: 0.2419\nBatch 180/270 | Loss: 0.3551\nBatch 190/270 | Loss: 0.2193\nBatch 200/270 | Loss: 0.4231\nBatch 210/270 | Loss: 0.3038\nBatch 220/270 | Loss: 0.3287\nBatch 230/270 | Loss: 0.1840\nBatch 240/270 | Loss: 0.4073\nBatch 250/270 | Loss: 0.5451\nBatch 260/270 | Loss: 0.3204\nBatch 270/270 | Loss: 0.3557\nEpoch 28 Loss: 0.3607 | Accuracy: 84.06%\n\nValidation Loss: 0.3497 | Validation Accuracy: 85.45%\n\nEpoch 29/50\n------------------------------\nBatch 10/270 | Loss: 0.6656\nBatch 20/270 | Loss: 0.3777\nBatch 30/270 | Loss: 0.3272\nBatch 40/270 | Loss: 0.4248\nBatch 50/270 | Loss: 0.2610\nBatch 60/270 | Loss: 0.4279\nBatch 70/270 | Loss: 0.3383\nBatch 80/270 | Loss: 0.3598\nBatch 90/270 | Loss: 0.2912\nBatch 100/270 | Loss: 0.3852\nBatch 110/270 | Loss: 0.4579\nBatch 120/270 | Loss: 0.3226\nBatch 130/270 | Loss: 0.3476\nBatch 140/270 | Loss: 0.3305\nBatch 150/270 | Loss: 0.4153\nBatch 160/270 | Loss: 0.3466\nBatch 170/270 | Loss: 0.1979\nBatch 180/270 | Loss: 0.4052\nBatch 190/270 | Loss: 0.2745\nBatch 200/270 | Loss: 0.4700\nBatch 210/270 | Loss: 0.3514\nBatch 220/270 | Loss: 0.3104\nBatch 230/270 | Loss: 0.3095\nBatch 240/270 | Loss: 0.1945\nBatch 250/270 | Loss: 0.3077\nBatch 260/270 | Loss: 0.3395\nBatch 270/270 | Loss: 0.5597\nEpoch 29 Loss: 0.3623 | Accuracy: 83.91%\n\nValidation Loss: 0.3407 | Validation Accuracy: 84.74%\n\nEpoch 30/50\n------------------------------\nBatch 10/270 | Loss: 0.3009\nBatch 20/270 | Loss: 0.3950\nBatch 30/270 | Loss: 0.2671\nBatch 40/270 | Loss: 0.5065\nBatch 50/270 | Loss: 0.4765\nBatch 60/270 | Loss: 0.4261\nBatch 70/270 | Loss: 0.3161\nBatch 80/270 | Loss: 0.5942\nBatch 90/270 | Loss: 0.3104\nBatch 100/270 | Loss: 0.2215\nBatch 110/270 | Loss: 0.3433\nBatch 120/270 | Loss: 0.2928\nBatch 130/270 | Loss: 0.4914\nBatch 140/270 | Loss: 0.2336\nBatch 150/270 | Loss: 0.2792\nBatch 160/270 | Loss: 0.3974\nBatch 170/270 | Loss: 0.5142\nBatch 180/270 | Loss: 0.3885\nBatch 190/270 | Loss: 0.3738\nBatch 200/270 | Loss: 0.3606\nBatch 210/270 | Loss: 0.2731\nBatch 220/270 | Loss: 0.3975\nBatch 230/270 | Loss: 0.2232\nBatch 240/270 | Loss: 0.3785\nBatch 250/270 | Loss: 0.3503\nBatch 260/270 | Loss: 0.2659\nBatch 270/270 | Loss: 0.2980\nEpoch 30 Loss: 0.3556 | Accuracy: 83.95%\n\nValidation Loss: 0.4080 | Validation Accuracy: 80.93%\n\nEpoch 31/50\n------------------------------\nBatch 10/270 | Loss: 0.2297\nBatch 20/270 | Loss: 0.4549\nBatch 30/270 | Loss: 0.3618\nBatch 40/270 | Loss: 0.3818\nBatch 50/270 | Loss: 0.3617\nBatch 60/270 | Loss: 0.3074\nBatch 70/270 | Loss: 0.3039\nBatch 80/270 | Loss: 0.5616\nBatch 90/270 | Loss: 0.5133\nBatch 100/270 | Loss: 0.2364\nBatch 110/270 | Loss: 0.2657\nBatch 120/270 | Loss: 0.2423\nBatch 130/270 | Loss: 0.4291\nBatch 140/270 | Loss: 0.2886\nBatch 150/270 | Loss: 0.3324\nBatch 160/270 | Loss: 0.3977\nBatch 170/270 | Loss: 0.1873\nBatch 180/270 | Loss: 0.2348\nBatch 190/270 | Loss: 0.3091\nBatch 200/270 | Loss: 0.4576\nBatch 210/270 | Loss: 0.4861\nBatch 220/270 | Loss: 0.4064\nBatch 230/270 | Loss: 0.2038\nBatch 240/270 | Loss: 0.3621\nBatch 250/270 | Loss: 0.2816\nBatch 260/270 | Loss: 0.3241\nBatch 270/270 | Loss: 0.2976\nEpoch 31 Loss: 0.3476 | Accuracy: 84.60%\n\nValidation Loss: 0.3672 | Validation Accuracy: 83.43%\n\nEpoch 32/50\n------------------------------\nBatch 10/270 | Loss: 0.3181\nBatch 20/270 | Loss: 0.3364\nBatch 30/270 | Loss: 0.3843\nBatch 40/270 | Loss: 0.3242\nBatch 50/270 | Loss: 0.1821\nBatch 60/270 | Loss: 0.3182\nBatch 70/270 | Loss: 0.2944\nBatch 80/270 | Loss: 0.4124\nBatch 90/270 | Loss: 0.3472\nBatch 100/270 | Loss: 0.6813\nBatch 110/270 | Loss: 0.2355\nBatch 120/270 | Loss: 0.2327\nBatch 130/270 | Loss: 0.1411\nBatch 140/270 | Loss: 0.3280\nBatch 150/270 | Loss: 0.3387\nBatch 160/270 | Loss: 0.1789\nBatch 170/270 | Loss: 0.4348\nBatch 180/270 | Loss: 0.3557\nBatch 190/270 | Loss: 0.3671\nBatch 200/270 | Loss: 0.3266\nBatch 210/270 | Loss: 0.2151\nBatch 220/270 | Loss: 0.3181\nBatch 230/270 | Loss: 0.3296\nBatch 240/270 | Loss: 0.3747\nBatch 250/270 | Loss: 0.3119\nBatch 260/270 | Loss: 0.2373\nBatch 270/270 | Loss: 0.3995\nEpoch 32 Loss: 0.3488 | Accuracy: 84.09%\n\nValidation Loss: 0.3562 | Validation Accuracy: 83.92%\n\nEpoch 33/50\n------------------------------\nBatch 10/270 | Loss: 0.4109\nBatch 20/270 | Loss: 0.3769\nBatch 30/270 | Loss: 0.4085\nBatch 40/270 | Loss: 0.3428\nBatch 50/270 | Loss: 0.5295\nBatch 60/270 | Loss: 0.4681\nBatch 70/270 | Loss: 0.5702\nBatch 80/270 | Loss: 0.3360\nBatch 90/270 | Loss: 0.2468\nBatch 100/270 | Loss: 0.3255\nBatch 110/270 | Loss: 0.2989\nBatch 120/270 | Loss: 0.1892\nBatch 130/270 | Loss: 0.3229\nBatch 140/270 | Loss: 0.2535\nBatch 150/270 | Loss: 0.5028\nBatch 160/270 | Loss: 0.2996\nBatch 170/270 | Loss: 0.3901\nBatch 180/270 | Loss: 0.3864\nBatch 190/270 | Loss: 0.3672\nBatch 200/270 | Loss: 0.2861\nBatch 210/270 | Loss: 0.3899\nBatch 220/270 | Loss: 0.4424\nBatch 230/270 | Loss: 0.3836\nBatch 240/270 | Loss: 0.3081\nBatch 250/270 | Loss: 0.2585\nBatch 260/270 | Loss: 0.2939\nBatch 270/270 | Loss: 0.4396\nEpoch 33 Loss: 0.3495 | Accuracy: 84.05%\n\nValidation Loss: 0.3616 | Validation Accuracy: 83.47%\n\nEpoch 34/50\n------------------------------\nBatch 10/270 | Loss: 0.2798\nBatch 20/270 | Loss: 0.2293\nBatch 30/270 | Loss: 0.3339\nBatch 40/270 | Loss: 0.1223\nBatch 50/270 | Loss: 0.3272\nBatch 60/270 | Loss: 0.3226\nBatch 70/270 | Loss: 0.4515\nBatch 80/270 | Loss: 0.4339\nBatch 90/270 | Loss: 0.3064\nBatch 100/270 | Loss: 0.2367\nBatch 110/270 | Loss: 0.2965\nBatch 120/270 | Loss: 0.4627\nBatch 130/270 | Loss: 0.3263\nBatch 140/270 | Loss: 0.4079\nBatch 150/270 | Loss: 0.4014\nBatch 160/270 | Loss: 0.5603\nBatch 170/270 | Loss: 0.3981\nBatch 180/270 | Loss: 0.4598\nBatch 190/270 | Loss: 0.4093\nBatch 200/270 | Loss: 0.3064\nBatch 210/270 | Loss: 0.3361\nBatch 220/270 | Loss: 0.3160\nBatch 230/270 | Loss: 0.2947\nBatch 240/270 | Loss: 0.4753\nBatch 250/270 | Loss: 0.3896\nBatch 260/270 | Loss: 0.3679\nBatch 270/270 | Loss: 0.2734\nEpoch 34 Loss: 0.3416 | Accuracy: 84.80%\n\nValidation Loss: 0.3341 | Validation Accuracy: 85.28%\n\nEpoch 35/50\n------------------------------\nBatch 10/270 | Loss: 0.3604\nBatch 20/270 | Loss: 0.3806\nBatch 30/270 | Loss: 0.3538\nBatch 40/270 | Loss: 0.2828\nBatch 50/270 | Loss: 0.2523\nBatch 60/270 | Loss: 0.5159\nBatch 70/270 | Loss: 0.4353\nBatch 80/270 | Loss: 0.2857\nBatch 90/270 | Loss: 0.3655\nBatch 100/270 | Loss: 0.2775\nBatch 110/270 | Loss: 0.2877\nBatch 120/270 | Loss: 0.4563\nBatch 130/270 | Loss: 0.4619\nBatch 140/270 | Loss: 0.2183\nBatch 150/270 | Loss: 0.4351\nBatch 160/270 | Loss: 0.1860\nBatch 170/270 | Loss: 0.3734\nBatch 180/270 | Loss: 0.3459\nBatch 190/270 | Loss: 0.4379\nBatch 200/270 | Loss: 0.2480\nBatch 210/270 | Loss: 0.3884\nBatch 220/270 | Loss: 0.3678\nBatch 230/270 | Loss: 0.4781\nBatch 240/270 | Loss: 0.1991\nBatch 250/270 | Loss: 0.3530\nBatch 260/270 | Loss: 0.2609\nBatch 270/270 | Loss: 0.3024\nEpoch 35 Loss: 0.3383 | Accuracy: 84.76%\n\nValidation Loss: 0.3351 | Validation Accuracy: 84.97%\n\nEpoch 36/50\n------------------------------\nBatch 10/270 | Loss: 0.2629\nBatch 20/270 | Loss: 0.4338\nBatch 30/270 | Loss: 0.2227\nBatch 40/270 | Loss: 0.4586\nBatch 50/270 | Loss: 0.5534\nBatch 60/270 | Loss: 0.4696\nBatch 70/270 | Loss: 0.2445\nBatch 80/270 | Loss: 0.4147\nBatch 90/270 | Loss: 0.2553\nBatch 100/270 | Loss: 0.3939\nBatch 110/270 | Loss: 0.4476\nBatch 120/270 | Loss: 0.2717\nBatch 130/270 | Loss: 0.2998\nBatch 140/270 | Loss: 0.3667\nBatch 150/270 | Loss: 0.2812\nBatch 160/270 | Loss: 0.1926\nBatch 170/270 | Loss: 0.2603\nBatch 180/270 | Loss: 0.2355\nBatch 190/270 | Loss: 0.3347\nBatch 200/270 | Loss: 0.3045\nBatch 210/270 | Loss: 0.3271\nBatch 220/270 | Loss: 0.4493\nBatch 230/270 | Loss: 0.3317\nBatch 240/270 | Loss: 0.2914\nBatch 250/270 | Loss: 0.3650\nBatch 260/270 | Loss: 0.3416\nBatch 270/270 | Loss: 0.3311\nEpoch 36 Loss: 0.3419 | Accuracy: 84.89%\n\nValidation Loss: 0.3247 | Validation Accuracy: 85.75%\n\nEpoch 37/50\n------------------------------\nBatch 10/270 | Loss: 0.2396\nBatch 20/270 | Loss: 0.2813\nBatch 30/270 | Loss: 0.3531\nBatch 40/270 | Loss: 0.3129\nBatch 50/270 | Loss: 0.3337\nBatch 60/270 | Loss: 0.3480\nBatch 70/270 | Loss: 0.3681\nBatch 80/270 | Loss: 0.3278\nBatch 90/270 | Loss: 0.2734\nBatch 100/270 | Loss: 0.3122\nBatch 110/270 | Loss: 0.4872\nBatch 120/270 | Loss: 0.3392\nBatch 130/270 | Loss: 0.2577\nBatch 140/270 | Loss: 0.3223\nBatch 150/270 | Loss: 0.3083\nBatch 160/270 | Loss: 0.3171\nBatch 170/270 | Loss: 0.3623\nBatch 180/270 | Loss: 0.5280\nBatch 190/270 | Loss: 0.3498\nBatch 200/270 | Loss: 0.2250\nBatch 210/270 | Loss: 0.4165\nBatch 220/270 | Loss: 0.2756\nBatch 230/270 | Loss: 0.2103\nBatch 240/270 | Loss: 0.4136\nBatch 250/270 | Loss: 0.1966\nBatch 260/270 | Loss: 0.4087\nBatch 270/270 | Loss: 0.5852\nEpoch 37 Loss: 0.3366 | Accuracy: 84.93%\n\nValidation Loss: 0.3439 | Validation Accuracy: 84.76%\n\nEpoch 38/50\n------------------------------\nBatch 10/270 | Loss: 0.3942\nBatch 20/270 | Loss: 0.3015\nBatch 30/270 | Loss: 0.3429\nBatch 40/270 | Loss: 0.3410\nBatch 50/270 | Loss: 0.4115\nBatch 60/270 | Loss: 0.3867\nBatch 70/270 | Loss: 0.3592\nBatch 80/270 | Loss: 0.1851\nBatch 90/270 | Loss: 0.2749\nBatch 100/270 | Loss: 0.3950\nBatch 110/270 | Loss: 0.2070\nBatch 120/270 | Loss: 0.1696\nBatch 130/270 | Loss: 0.1591\nBatch 140/270 | Loss: 0.3857\nBatch 150/270 | Loss: 0.3875\nBatch 160/270 | Loss: 0.2095\nBatch 170/270 | Loss: 0.2827\nBatch 180/270 | Loss: 0.3908\nBatch 190/270 | Loss: 0.4135\nBatch 200/270 | Loss: 0.5717\nBatch 210/270 | Loss: 0.4364\nBatch 220/270 | Loss: 0.3380\nBatch 230/270 | Loss: 0.4823\nBatch 240/270 | Loss: 0.2422\nBatch 250/270 | Loss: 0.3334\nBatch 260/270 | Loss: 0.2620\nBatch 270/270 | Loss: 0.3723\nEpoch 38 Loss: 0.3350 | Accuracy: 84.85%\n\nValidation Loss: 0.3412 | Validation Accuracy: 84.46%\n\nEpoch 39/50\n------------------------------\nBatch 10/270 | Loss: 0.4983\nBatch 20/270 | Loss: 0.3652\nBatch 30/270 | Loss: 0.5199\nBatch 40/270 | Loss: 0.4506\nBatch 50/270 | Loss: 0.2756\nBatch 60/270 | Loss: 0.2805\nBatch 70/270 | Loss: 0.4112\nBatch 80/270 | Loss: 0.1696\nBatch 90/270 | Loss: 0.2800\nBatch 100/270 | Loss: 0.2222\nBatch 110/270 | Loss: 0.2606\nBatch 120/270 | Loss: 0.2614\nBatch 130/270 | Loss: 0.4391\nBatch 140/270 | Loss: 0.2076\nBatch 150/270 | Loss: 0.4265\nBatch 160/270 | Loss: 0.3097\nBatch 170/270 | Loss: 0.3313\nBatch 180/270 | Loss: 0.2656\nBatch 190/270 | Loss: 0.3733\nBatch 200/270 | Loss: 0.4662\nBatch 210/270 | Loss: 0.2903\nBatch 220/270 | Loss: 0.1873\nBatch 230/270 | Loss: 0.2350\nBatch 240/270 | Loss: 0.3108\nBatch 250/270 | Loss: 0.3263\nBatch 260/270 | Loss: 0.3283\nBatch 270/270 | Loss: 0.2742\nEpoch 39 Loss: 0.3382 | Accuracy: 84.64%\n\nValidation Loss: 0.3398 | Validation Accuracy: 84.74%\n\nEpoch 40/50\n------------------------------\nBatch 10/270 | Loss: 0.3451\nBatch 20/270 | Loss: 0.2093\nBatch 30/270 | Loss: 0.4138\nBatch 40/270 | Loss: 0.2837\nBatch 50/270 | Loss: 0.4211\nBatch 60/270 | Loss: 0.2690\nBatch 70/270 | Loss: 0.3030\nBatch 80/270 | Loss: 0.2344\nBatch 90/270 | Loss: 0.2460\nBatch 100/270 | Loss: 0.1766\nBatch 110/270 | Loss: 0.2533\nBatch 120/270 | Loss: 0.5082\nBatch 130/270 | Loss: 0.3909\nBatch 140/270 | Loss: 0.3890\nBatch 150/270 | Loss: 0.3719\nBatch 160/270 | Loss: 0.4025\nBatch 170/270 | Loss: 0.1662\nBatch 180/270 | Loss: 0.4526\nBatch 190/270 | Loss: 0.4627\nBatch 200/270 | Loss: 0.2479\nBatch 210/270 | Loss: 0.2228\nBatch 220/270 | Loss: 0.2823\nBatch 230/270 | Loss: 0.3820\nBatch 240/270 | Loss: 0.3128\nBatch 250/270 | Loss: 0.2829\nBatch 260/270 | Loss: 0.3007\nBatch 270/270 | Loss: 0.3012\nEpoch 40 Loss: 0.3273 | Accuracy: 85.28%\n\nValidation Loss: 0.3323 | Validation Accuracy: 85.16%\n\nEpoch 41/50\n------------------------------\nBatch 10/270 | Loss: 0.4402\nBatch 20/270 | Loss: 0.3443\nBatch 30/270 | Loss: 0.2954\nBatch 40/270 | Loss: 0.2643\nBatch 50/270 | Loss: 0.3635\nBatch 60/270 | Loss: 0.2332\nBatch 70/270 | Loss: 0.1791\nBatch 80/270 | Loss: 0.3046\nBatch 90/270 | Loss: 0.3340\nBatch 100/270 | Loss: 0.2674\nBatch 110/270 | Loss: 0.1336\nBatch 120/270 | Loss: 0.2429\nBatch 130/270 | Loss: 0.3933\nBatch 140/270 | Loss: 0.2827\nBatch 150/270 | Loss: 0.2092\nBatch 160/270 | Loss: 0.2192\nBatch 170/270 | Loss: 0.2439\nBatch 180/270 | Loss: 0.3378\nBatch 190/270 | Loss: 0.2140\nBatch 200/270 | Loss: 0.3021\nBatch 210/270 | Loss: 0.2615\nBatch 220/270 | Loss: 0.4047\nBatch 230/270 | Loss: 0.2991\nBatch 240/270 | Loss: 0.3121\nBatch 250/270 | Loss: 0.2949\nBatch 260/270 | Loss: 0.3125\nBatch 270/270 | Loss: 0.2230\nEpoch 41 Loss: 0.3172 | Accuracy: 85.89%\n\nValidation Loss: 0.3367 | Validation Accuracy: 84.79%\n\nEpoch 42/50\n------------------------------\nBatch 10/270 | Loss: 0.4375\nBatch 20/270 | Loss: 0.2695\nBatch 30/270 | Loss: 0.2171\nBatch 40/270 | Loss: 0.3070\nBatch 50/270 | Loss: 0.1881\nBatch 60/270 | Loss: 0.4285\nBatch 70/270 | Loss: 0.2718\nBatch 80/270 | Loss: 0.3283\nBatch 90/270 | Loss: 0.3386\nBatch 100/270 | Loss: 0.3611\nBatch 110/270 | Loss: 0.3414\nBatch 120/270 | Loss: 0.2499\nBatch 130/270 | Loss: 0.3654\nBatch 140/270 | Loss: 0.5302\nBatch 150/270 | Loss: 0.2958\nBatch 160/270 | Loss: 0.2250\nBatch 170/270 | Loss: 0.3405\nBatch 180/270 | Loss: 0.2602\nBatch 190/270 | Loss: 0.2222\nBatch 200/270 | Loss: 0.3342\nBatch 210/270 | Loss: 0.4606\nBatch 220/270 | Loss: 0.6149\nBatch 230/270 | Loss: 0.5319\nBatch 240/270 | Loss: 0.2163\nBatch 250/270 | Loss: 0.4019\nBatch 260/270 | Loss: 0.3265\nBatch 270/270 | Loss: 0.2421\nEpoch 42 Loss: 0.3269 | Accuracy: 85.30%\n\nValidation Loss: 0.3100 | Validation Accuracy: 85.30%\n\nEpoch 43/50\n------------------------------\nBatch 10/270 | Loss: 0.3515\nBatch 20/270 | Loss: 0.3052\nBatch 30/270 | Loss: 0.5209\nBatch 40/270 | Loss: 0.5745\nBatch 50/270 | Loss: 0.2907\nBatch 60/270 | Loss: 0.2496\nBatch 70/270 | Loss: 0.3294\nBatch 80/270 | Loss: 0.2680\nBatch 90/270 | Loss: 0.3417\nBatch 100/270 | Loss: 0.2882\nBatch 110/270 | Loss: 0.3077\nBatch 120/270 | Loss: 0.2607\nBatch 130/270 | Loss: 0.3077\nBatch 140/270 | Loss: 0.3489\nBatch 150/270 | Loss: 0.3749\nBatch 160/270 | Loss: 0.2711\nBatch 170/270 | Loss: 0.3607\nBatch 180/270 | Loss: 0.2646\nBatch 190/270 | Loss: 0.2831\nBatch 200/270 | Loss: 0.3118\nBatch 210/270 | Loss: 0.3303\nBatch 220/270 | Loss: 0.3780\nBatch 230/270 | Loss: 0.2126\nBatch 240/270 | Loss: 0.3810\nBatch 250/270 | Loss: 0.1972\nBatch 260/270 | Loss: 0.3782\nBatch 270/270 | Loss: 0.2318\nEpoch 43 Loss: 0.3189 | Accuracy: 85.71%\n\nValidation Loss: 0.3016 | Validation Accuracy: 86.43%\n\nEpoch 44/50\n------------------------------\nBatch 10/270 | Loss: 0.2614\nBatch 20/270 | Loss: 0.3556\nBatch 30/270 | Loss: 0.1915\nBatch 40/270 | Loss: 0.3619\nBatch 50/270 | Loss: 0.3269\nBatch 60/270 | Loss: 0.3643\nBatch 70/270 | Loss: 0.2580\nBatch 80/270 | Loss: 0.2708\nBatch 90/270 | Loss: 0.2526\nBatch 100/270 | Loss: 0.2759\nBatch 110/270 | Loss: 0.4052\nBatch 120/270 | Loss: 0.4363\nBatch 130/270 | Loss: 0.3717\nBatch 140/270 | Loss: 0.4212\nBatch 150/270 | Loss: 0.3160\nBatch 160/270 | Loss: 0.4787\nBatch 170/270 | Loss: 0.1919\nBatch 180/270 | Loss: 0.1722\nBatch 190/270 | Loss: 0.3929\nBatch 200/270 | Loss: 0.3362\nBatch 210/270 | Loss: 0.3820\nBatch 220/270 | Loss: 0.3332\nBatch 230/270 | Loss: 0.3460\nBatch 240/270 | Loss: 0.4098\nBatch 250/270 | Loss: 0.3180\nBatch 260/270 | Loss: 0.2312\nBatch 270/270 | Loss: 0.0643\nEpoch 44 Loss: 0.3170 | Accuracy: 85.62%\n\nValidation Loss: 0.3117 | Validation Accuracy: 86.20%\n\nEpoch 45/50\n------------------------------\nBatch 10/270 | Loss: 0.3448\nBatch 20/270 | Loss: 0.1502\nBatch 30/270 | Loss: 0.3599\nBatch 40/270 | Loss: 0.4066\nBatch 50/270 | Loss: 0.1952\nBatch 60/270 | Loss: 0.3427\nBatch 70/270 | Loss: 0.3438\nBatch 80/270 | Loss: 0.3209\nBatch 90/270 | Loss: 0.3488\nBatch 100/270 | Loss: 0.4803\nBatch 110/270 | Loss: 0.3070\nBatch 120/270 | Loss: 0.2984\nBatch 130/270 | Loss: 0.2170\nBatch 140/270 | Loss: 0.1965\nBatch 150/270 | Loss: 0.3452\nBatch 160/270 | Loss: 0.2089\nBatch 170/270 | Loss: 0.3573\nBatch 180/270 | Loss: 0.2780\nBatch 190/270 | Loss: 0.2788\nBatch 200/270 | Loss: 0.3651\nBatch 210/270 | Loss: 0.3647\nBatch 220/270 | Loss: 0.2444\nBatch 230/270 | Loss: 0.1981\nBatch 240/270 | Loss: 0.2836\nBatch 250/270 | Loss: 0.2589\nBatch 260/270 | Loss: 0.3530\nBatch 270/270 | Loss: 0.2199\nEpoch 45 Loss: 0.3146 | Accuracy: 85.72%\n\nValidation Loss: 0.3107 | Validation Accuracy: 86.29%\n\nEpoch 46/50\n------------------------------\nBatch 10/270 | Loss: 0.4678\nBatch 20/270 | Loss: 0.4062\nBatch 30/270 | Loss: 0.1665\nBatch 40/270 | Loss: 0.3547\nBatch 50/270 | Loss: 0.3252\nBatch 60/270 | Loss: 0.2381\nBatch 70/270 | Loss: 0.2959\nBatch 80/270 | Loss: 0.3841\nBatch 90/270 | Loss: 0.3373\nBatch 100/270 | Loss: 0.3489\nBatch 110/270 | Loss: 0.1876\nBatch 120/270 | Loss: 0.3099\nBatch 130/270 | Loss: 0.2821\nBatch 140/270 | Loss: 0.4789\nBatch 150/270 | Loss: 0.3986\nBatch 160/270 | Loss: 0.2438\nBatch 170/270 | Loss: 0.2219\nBatch 180/270 | Loss: 0.3054\nBatch 190/270 | Loss: 0.2668\nBatch 200/270 | Loss: 0.2469\nBatch 210/270 | Loss: 0.3274\nBatch 220/270 | Loss: 0.4247\nBatch 230/270 | Loss: 0.1803\nBatch 240/270 | Loss: 0.2640\nBatch 250/270 | Loss: 0.3764\nBatch 260/270 | Loss: 0.3888\nBatch 270/270 | Loss: 0.0964\nEpoch 46 Loss: 0.3176 | Accuracy: 85.76%\n\nValidation Loss: 0.3060 | Validation Accuracy: 86.45%\n\nEpoch 47/50\n------------------------------\nBatch 10/270 | Loss: 0.3385\nBatch 20/270 | Loss: 0.1728\nBatch 30/270 | Loss: 0.3122\nBatch 40/270 | Loss: 0.2360\nBatch 50/270 | Loss: 0.2879\nBatch 60/270 | Loss: 0.3893\nBatch 70/270 | Loss: 0.3326\nBatch 80/270 | Loss: 0.3652\nBatch 90/270 | Loss: 0.4614\nBatch 100/270 | Loss: 0.3823\nBatch 110/270 | Loss: 0.4765\nBatch 120/270 | Loss: 0.3367\nBatch 130/270 | Loss: 0.1840\nBatch 140/270 | Loss: 0.3869\nBatch 150/270 | Loss: 0.2674\nBatch 160/270 | Loss: 0.3447\nBatch 170/270 | Loss: 0.2887\nBatch 180/270 | Loss: 0.2232\nBatch 190/270 | Loss: 0.3080\nBatch 200/270 | Loss: 0.2854\nBatch 210/270 | Loss: 0.4012\nBatch 220/270 | Loss: 0.2932\nBatch 230/270 | Loss: 0.3863\nBatch 240/270 | Loss: 0.4763\nBatch 250/270 | Loss: 0.3764\nBatch 260/270 | Loss: 0.2960\nBatch 270/270 | Loss: 0.2516\nEpoch 47 Loss: 0.3130 | Accuracy: 85.89%\n\nValidation Loss: 0.3055 | Validation Accuracy: 86.43%\n\nEpoch 48/50\n------------------------------\nBatch 10/270 | Loss: 0.2795\nBatch 20/270 | Loss: 0.2528\nBatch 30/270 | Loss: 0.4916\nBatch 40/270 | Loss: 0.2814\nBatch 50/270 | Loss: 0.2154\nBatch 60/270 | Loss: 0.2388\nBatch 70/270 | Loss: 0.4955\nBatch 80/270 | Loss: 0.1516\nBatch 90/270 | Loss: 0.1847\nBatch 100/270 | Loss: 0.1233\nBatch 110/270 | Loss: 0.3319\nBatch 120/270 | Loss: 0.3542\nBatch 130/270 | Loss: 0.3113\nBatch 140/270 | Loss: 0.3239\nBatch 150/270 | Loss: 0.2157\nBatch 160/270 | Loss: 0.3552\nBatch 170/270 | Loss: 0.4259\nBatch 180/270 | Loss: 0.2872\nBatch 190/270 | Loss: 0.4329\nBatch 200/270 | Loss: 0.2217\nBatch 210/270 | Loss: 0.3973\nBatch 220/270 | Loss: 0.2158\nBatch 230/270 | Loss: 0.3017\nBatch 240/270 | Loss: 0.3056\nBatch 250/270 | Loss: 0.2332\nBatch 260/270 | Loss: 0.2659\nBatch 270/270 | Loss: 0.0901\nEpoch 48 Loss: 0.2983 | Accuracy: 86.45%\n\nValidation Loss: 0.3128 | Validation Accuracy: 86.67%\n\nEpoch 49/50\n------------------------------\nBatch 10/270 | Loss: 0.1807\nBatch 20/270 | Loss: 0.2856\nBatch 30/270 | Loss: 0.3225\nBatch 40/270 | Loss: 0.3009\nBatch 50/270 | Loss: 0.4941\nBatch 60/270 | Loss: 0.4709\nBatch 70/270 | Loss: 0.2466\nBatch 80/270 | Loss: 0.2091\nBatch 90/270 | Loss: 0.1709\nBatch 100/270 | Loss: 0.3225\nBatch 110/270 | Loss: 0.3579\nBatch 120/270 | Loss: 0.2340\nBatch 130/270 | Loss: 0.3031\nBatch 140/270 | Loss: 0.1893\nBatch 150/270 | Loss: 0.4565\nBatch 160/270 | Loss: 0.3127\nBatch 170/270 | Loss: 0.1528\nBatch 180/270 | Loss: 0.2891\nBatch 190/270 | Loss: 0.2407\nBatch 200/270 | Loss: 0.2702\nBatch 210/270 | Loss: 0.3267\nBatch 220/270 | Loss: 0.4397\nBatch 230/270 | Loss: 0.2716\nBatch 240/270 | Loss: 0.3637\nBatch 250/270 | Loss: 0.5857\nBatch 260/270 | Loss: 0.3041\nBatch 270/270 | Loss: 0.2906\nEpoch 49 Loss: 0.3156 | Accuracy: 85.42%\n\nValidation Loss: 0.3030 | Validation Accuracy: 86.57%\n\nEpoch 50/50\n------------------------------\nBatch 10/270 | Loss: 0.1703\nBatch 20/270 | Loss: 0.1751\nBatch 30/270 | Loss: 0.2326\nBatch 40/270 | Loss: 0.2242\nBatch 50/270 | Loss: 0.4174\nBatch 60/270 | Loss: 0.1734\nBatch 70/270 | Loss: 0.4325\nBatch 80/270 | Loss: 0.4520\nBatch 90/270 | Loss: 0.2309\nBatch 100/270 | Loss: 0.2680\nBatch 110/270 | Loss: 0.0929\nBatch 120/270 | Loss: 0.4287\nBatch 130/270 | Loss: 0.3078\nBatch 140/270 | Loss: 0.1427\nBatch 150/270 | Loss: 0.4230\nBatch 160/270 | Loss: 0.3015\nBatch 170/270 | Loss: 0.0903\nBatch 180/270 | Loss: 0.3122\nBatch 190/270 | Loss: 0.2464\nBatch 200/270 | Loss: 0.2688\nBatch 210/270 | Loss: 0.2452\nBatch 220/270 | Loss: 0.3976\nBatch 230/270 | Loss: 0.2050\nBatch 240/270 | Loss: 0.2209\nBatch 250/270 | Loss: 0.2712\nBatch 260/270 | Loss: 0.6220\nBatch 270/270 | Loss: 0.1912\nEpoch 50 Loss: 0.3001 | Accuracy: 86.47%\n\nValidation Loss: 0.2976 | Validation Accuracy: 86.71%\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Training loop\ndef train_model(model, dataloaders, criterion, optimizer, num_epochs=10):\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(\"-\" * 30)\n        \n        for batch_idx, (inputs, labels) in enumerate(dataloaders['train']):\n            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            if (batch_idx + 1) % 10 == 0:\n                print(f\"Batch {batch_idx+1}/{len(dataloaders['train'])} | Loss: {loss.item():.4f}\")\n        \n        epoch_loss = running_loss / len(dataloaders['train'])\n        epoch_acc = 100. * correct / total\n        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%\\n\")\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        val_correct = 0\n        val_total = 0\n        with torch.no_grad():\n            for inputs, labels in dataloaders['val']:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = outputs.max(1)\n                val_total += labels.size(0)\n                val_correct += predicted.eq(labels).sum().item()\n        \n        val_loss /= len(dataloaders['val'])\n        val_acc = 100. * val_correct / val_total\n        print(f\"Validation Loss: {val_loss:.4f} | Validation Accuracy: {val_acc:.2f}%\\n\")\n\ntrain_model(model, dataloaders, criterion, optimizer, num_epochs=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T16:02:11.031878Z","iopub.execute_input":"2025-03-23T16:02:11.032295Z","iopub.status.idle":"2025-03-23T16:28:29.999339Z","shell.execute_reply.started":"2025-03-23T16:02:11.032264Z","shell.execute_reply":"2025-03-23T16:28:29.998354Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\n------------------------------\nBatch 10/270 | Loss: 0.3293\nBatch 20/270 | Loss: 0.2029\nBatch 30/270 | Loss: 0.2180\nBatch 40/270 | Loss: 0.1726\nBatch 50/270 | Loss: 0.3503\nBatch 60/270 | Loss: 0.4824\nBatch 70/270 | Loss: 0.2497\nBatch 80/270 | Loss: 0.2939\nBatch 90/270 | Loss: 0.4331\nBatch 100/270 | Loss: 0.1668\nBatch 110/270 | Loss: 0.2951\nBatch 120/270 | Loss: 0.2473\nBatch 130/270 | Loss: 0.1971\nBatch 140/270 | Loss: 0.3398\nBatch 150/270 | Loss: 0.4505\nBatch 160/270 | Loss: 0.2370\nBatch 170/270 | Loss: 0.2318\nBatch 180/270 | Loss: 0.3375\nBatch 190/270 | Loss: 0.3058\nBatch 200/270 | Loss: 0.1428\nBatch 210/270 | Loss: 0.2063\nBatch 220/270 | Loss: 0.1731\nBatch 230/270 | Loss: 0.3268\nBatch 240/270 | Loss: 0.1944\nBatch 250/270 | Loss: 0.3154\nBatch 260/270 | Loss: 0.4840\nBatch 270/270 | Loss: 0.7212\nEpoch 1 Loss: 0.2868 | Accuracy: 87.50%\n\nValidation Loss: 0.2953 | Validation Accuracy: 86.36%\n\nEpoch 2/5\n------------------------------\nBatch 10/270 | Loss: 0.2503\nBatch 20/270 | Loss: 0.2521\nBatch 30/270 | Loss: 0.3589\nBatch 40/270 | Loss: 0.2860\nBatch 50/270 | Loss: 0.2715\nBatch 60/270 | Loss: 0.2915\nBatch 70/270 | Loss: 0.2189\nBatch 80/270 | Loss: 0.4591\nBatch 90/270 | Loss: 0.1551\nBatch 100/270 | Loss: 0.2053\nBatch 110/270 | Loss: 0.1783\nBatch 120/270 | Loss: 0.3049\nBatch 130/270 | Loss: 0.2503\nBatch 140/270 | Loss: 0.2097\nBatch 150/270 | Loss: 0.2842\nBatch 160/270 | Loss: 0.2218\nBatch 170/270 | Loss: 0.4332\nBatch 180/270 | Loss: 0.2261\nBatch 190/270 | Loss: 0.3764\nBatch 200/270 | Loss: 0.4083\nBatch 210/270 | Loss: 0.3239\nBatch 220/270 | Loss: 0.3356\nBatch 230/270 | Loss: 0.4798\nBatch 240/270 | Loss: 0.3077\nBatch 250/270 | Loss: 0.3132\nBatch 260/270 | Loss: 0.3339\nBatch 270/270 | Loss: 0.2648\nEpoch 2 Loss: 0.2861 | Accuracy: 87.47%\n\nValidation Loss: 0.2712 | Validation Accuracy: 88.24%\n\nEpoch 3/5\n------------------------------\nBatch 10/270 | Loss: 0.3333\nBatch 20/270 | Loss: 0.3848\nBatch 30/270 | Loss: 0.3211\nBatch 40/270 | Loss: 0.2644\nBatch 50/270 | Loss: 0.1189\nBatch 60/270 | Loss: 0.5029\nBatch 70/270 | Loss: 0.3509\nBatch 80/270 | Loss: 0.1834\nBatch 90/270 | Loss: 0.2080\nBatch 100/270 | Loss: 0.1847\nBatch 110/270 | Loss: 0.3053\nBatch 120/270 | Loss: 0.0886\nBatch 130/270 | Loss: 0.2810\nBatch 140/270 | Loss: 0.2884\nBatch 150/270 | Loss: 0.5688\nBatch 160/270 | Loss: 0.3552\nBatch 170/270 | Loss: 0.2065\nBatch 180/270 | Loss: 0.2576\nBatch 190/270 | Loss: 0.3453\nBatch 200/270 | Loss: 0.2444\nBatch 210/270 | Loss: 0.3075\nBatch 220/270 | Loss: 0.2778\nBatch 230/270 | Loss: 0.3443\nBatch 240/270 | Loss: 0.2162\nBatch 250/270 | Loss: 0.2291\nBatch 260/270 | Loss: 0.2144\nBatch 270/270 | Loss: 0.8605\nEpoch 3 Loss: 0.2853 | Accuracy: 87.80%\n\nValidation Loss: 0.2675 | Validation Accuracy: 88.15%\n\nEpoch 4/5\n------------------------------\nBatch 10/270 | Loss: 0.2342\nBatch 20/270 | Loss: 0.2744\nBatch 30/270 | Loss: 0.2272\nBatch 40/270 | Loss: 0.3030\nBatch 50/270 | Loss: 0.3877\nBatch 60/270 | Loss: 0.2743\nBatch 70/270 | Loss: 0.1970\nBatch 80/270 | Loss: 0.1349\nBatch 90/270 | Loss: 0.4789\nBatch 100/270 | Loss: 0.3949\nBatch 110/270 | Loss: 0.3035\nBatch 120/270 | Loss: 0.5288\nBatch 130/270 | Loss: 0.3439\nBatch 140/270 | Loss: 0.2630\nBatch 150/270 | Loss: 0.3027\nBatch 160/270 | Loss: 0.2434\nBatch 170/270 | Loss: 0.2722\nBatch 180/270 | Loss: 0.1395\nBatch 190/270 | Loss: 0.3322\nBatch 200/270 | Loss: 0.1953\nBatch 210/270 | Loss: 0.2483\nBatch 220/270 | Loss: 0.3231\nBatch 230/270 | Loss: 0.2763\nBatch 240/270 | Loss: 0.3734\nBatch 250/270 | Loss: 0.1739\nBatch 260/270 | Loss: 0.3094\nBatch 270/270 | Loss: 0.3673\nEpoch 4 Loss: 0.2793 | Accuracy: 87.96%\n\nValidation Loss: 0.2947 | Validation Accuracy: 87.11%\n\nEpoch 5/5\n------------------------------\nBatch 10/270 | Loss: 0.3906\nBatch 20/270 | Loss: 0.3509\nBatch 30/270 | Loss: 0.3883\nBatch 40/270 | Loss: 0.2062\nBatch 50/270 | Loss: 0.3247\nBatch 60/270 | Loss: 0.3459\nBatch 70/270 | Loss: 0.2059\nBatch 80/270 | Loss: 0.2833\nBatch 90/270 | Loss: 0.3569\nBatch 100/270 | Loss: 0.2256\nBatch 110/270 | Loss: 0.2404\nBatch 120/270 | Loss: 0.2241\nBatch 130/270 | Loss: 0.2815\nBatch 140/270 | Loss: 0.2267\nBatch 150/270 | Loss: 0.4450\nBatch 160/270 | Loss: 0.3320\nBatch 170/270 | Loss: 0.3565\nBatch 180/270 | Loss: 0.2119\nBatch 190/270 | Loss: 0.3277\nBatch 200/270 | Loss: 0.2736\nBatch 210/270 | Loss: 0.2548\nBatch 220/270 | Loss: 0.3564\nBatch 230/270 | Loss: 0.2847\nBatch 240/270 | Loss: 0.3033\nBatch 250/270 | Loss: 0.3122\nBatch 260/270 | Loss: 0.2620\nBatch 270/270 | Loss: 0.1782\nEpoch 5 Loss: 0.2796 | Accuracy: 87.44%\n\nValidation Loss: 0.3011 | Validation Accuracy: 86.79%\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"def evaluate_model(model, dataloader, criterion):\n    model.eval()\n    total = 0\n    correct = 0\n    running_loss = 0.0\n    \n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    test_loss = running_loss / len(dataloader)\n    test_acc = 100. * correct / total\n    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n\nevaluate_model(model, dataloaders['test'], criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T16:28:55.475035Z","iopub.execute_input":"2025-03-23T16:28:55.475384Z","iopub.status.idle":"2025-03-23T16:30:10.642354Z","shell.execute_reply.started":"2025-03-23T16:28:55.475356Z","shell.execute_reply":"2025-03-23T16:30:10.641152Z"}},"outputs":[{"name":"stdout","text":"Test Loss: 0.3144 | Test Accuracy: 86.08%\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"torch.save(model.state_dict(), \"glaucoma_model_SNN_86TA.pth\")\nprint(\"Model saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T16:30:25.872986Z","iopub.execute_input":"2025-03-23T16:30:25.873308Z","iopub.status.idle":"2025-03-23T16:30:25.911330Z","shell.execute_reply.started":"2025-03-23T16:30:25.873285Z","shell.execute_reply":"2025-03-23T16:30:25.910655Z"}},"outputs":[{"name":"stdout","text":"Model saved successfully!\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}